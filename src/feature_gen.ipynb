{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from python_speech_features import logfbank, mfcc\n",
    "\n",
    "import scipy.io.wavfile\n",
    "import sys\n",
    "import os\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DATASET PATH (RELATIVE) (not cross-OS who cares about shitty OS)\n",
    "dataset_path = '../dataset/'\n",
    "## Here we are interested in averaged dynamic annotations for both arousal and valence :\n",
    "annotations_path = 'annotations/'\n",
    "rating_mode = 'averaged_per_song/' #(per_each_rater/averaged_per_song)\n",
    "time_continuity_mode = 'dynamic/'#(song_level/dynamic)\n",
    "full_data_path = dataset_path + annotations_path + rating_mode + time_continuity_mode\n",
    "## Here is the path to the audio recordings :\n",
    "audio_path = 'MEMD_audio/'\n",
    "full_audio_path = dataset_path + audio_path\n",
    "## Here is the path to where we write the csv\n",
    "csv_write_path = dataset_path + 'emotion_by_song/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = '../features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['song_id', 'sample_15000ms', 'sample_15500ms', 'sample_16000ms',\n",
       "       'sample_16500ms', 'sample_17000ms', 'sample_17500ms', 'sample_18000ms',\n",
       "       'sample_18500ms', 'sample_19000ms',\n",
       "       ...\n",
       "       'sample_621500ms', 'sample_622000ms', 'sample_622500ms',\n",
       "       'sample_623000ms', 'sample_623500ms', 'sample_624000ms',\n",
       "       'sample_624500ms', 'sample_625000ms', 'sample_625500ms',\n",
       "       'sample_626000ms'],\n",
       "      dtype='object', length=1224)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arousal data loading\n",
    "arousal_dataframe = pd.read_csv(full_data_path + 'arousal' + '.csv')\n",
    "arousal_dataframe.keys()\n",
    "arousal_dataframe = arousal_dataframe.drop(columns='sample_626500ms')\n",
    "arousal_dataframe.keys()\n",
    "#print(arousal_dataframe.shape)\n",
    "#print(arousal_dataframe['song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['song_id', 'sample_15000ms', 'sample_15500ms', 'sample_16000ms',\n",
       "       'sample_16500ms', 'sample_17000ms', 'sample_17500ms', 'sample_18000ms',\n",
       "       'sample_18500ms', 'sample_19000ms',\n",
       "       ...\n",
       "       'sample_621500ms', 'sample_622000ms', 'sample_622500ms',\n",
       "       'sample_623000ms', 'sample_623500ms', 'sample_624000ms',\n",
       "       'sample_624500ms', 'sample_625000ms', 'sample_625500ms',\n",
       "       'sample_626000ms'],\n",
       "      dtype='object', length=1224)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence_dataframe = pd.read_csv(full_data_path + 'valence' + '.csv')\n",
    "valence_dataframe.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotional_trajectory(song_id):\n",
    "    valence_id = (valence_dataframe.loc[valence_dataframe['song_id'] == song_id]\n",
    "                                   .drop(columns='song_id')\n",
    "                                   .values\n",
    "                 )\n",
    "    valence_id = np.ravel(valence_id)\n",
    "    \n",
    "    arousal_id = (arousal_dataframe.loc[arousal_dataframe['song_id'] == song_id]\n",
    "                                   .drop(columns='song_id')\n",
    "                                   .values\n",
    "                 )\n",
    "    arousal_id = np.ravel(arousal_id)\n",
    "    plt.figure()\n",
    "    plt.plot(valence_id, arousal_id)\n",
    "    plt.show()\n",
    "\n",
    "def compute_mfb_coef(sig, sample_rate):\n",
    "    return logfbank(signal = sig,\n",
    "                    samplerate=sample_rate,\n",
    "                    winlen=0.020,\n",
    "                    winstep=0.010,\n",
    "                    nfilt=40,\n",
    "                    nfft=512,\n",
    "                    lowfreq=0,\n",
    "                    highfreq=None,\n",
    "                    preemph=0.97)\n",
    "\n",
    "def slice_signal_by_time_steps(sig, sample_rate, time_step):\n",
    "    index_step = int(time_step*sample_rate)\n",
    "    n_time_steps = sig.shape[0]//index_step\n",
    "    return sig[:n_time_steps*index_step].reshape((n_time_steps, index_step))\n",
    "\n",
    "def audio_from_song_id(song_id):\n",
    "        file_path = full_audio_path + str(song_id) + '.mp3'\n",
    "        return librosa.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9297ccc9cdc6425da455f13681fbe281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1802), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for i in tqdm_notebook(range(arousal_dataframe.shape[0])):\n",
    "    arousal_line = arousal_dataframe.loc[i]\n",
    "    valence_line = valence_dataframe.loc[i]\n",
    "    song_id_ar = int(arousal_line['song_id'])\n",
    "    song_id_val = int(valence_line['song_id'])\n",
    "    if song_id_ar == song_id_val:\n",
    "        #Load the corresponding signal\n",
    "        y, sampling_rate = audio_from_song_id(song_id_ar)\n",
    "        #format it so that each row of the resulting matrix correspond to a 500 ms signal duration\n",
    "        formatted_signal = slice_signal_by_time_steps(y, sampling_rate, 500*1e-3)\n",
    "        #Remove the 30 first rows for which we don't have arousal/valence information\n",
    "        formatted_signal = formatted_signal[30:,:]\n",
    "        M = formatted_signal.shape[0]\n",
    "        n_mfb = 40\n",
    "        n_time_win = 49\n",
    "        mfb_coefficients = np.zeros((M, n_time_win, n_mfb))\n",
    "        for i in range(M):\n",
    "            mfb_coefficients[i,:,:] = compute_mfb_coef(formatted_signal[i], sampling_rate)\n",
    "        \n",
    "        mfb_coefficients -= np.mean(np.mean(mfb_coefficients, axis=0), axis=0)\n",
    "        mfb_coefficients = mfb_coefficients.reshape(M, n_mfb*n_time_win)\n",
    "        \n",
    "        arousal_line_vector = arousal_line.drop('song_id').values[:M]\n",
    "        valence_line_vector = valence_line.drop('song_id').values[:M]\n",
    "        \n",
    "        emotional_vector = np.stack([arousal_line_vector, valence_line_vector]).T\n",
    "        \n",
    "        #save results\n",
    "        outputfile = feature_path + 'song_' + str(song_id_ar) + '.npz'\n",
    "        np.savez(outputfile, mfb = mfb_coefficients, emotion = emotional_vector)\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
