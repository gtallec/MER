{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from python_speech_features import logfbank, mfcc\n",
    "\n",
    "import scipy.io.wavfile\n",
    "import scipy.interpolate\n",
    "import sys\n",
    "import os\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DATASET PATH (RELATIVE) (not cross-OS who cares about shitty OS)\n",
    "dataset_path = '../dataset/'\n",
    "## Here we are interested in averaged dynamic annotations for both arousal and valence :\n",
    "annotations_path = 'annotations/'\n",
    "rating_mode = 'per_each_rater/' #(per_each_rater/averaged_per_song)\n",
    "time_continuity_mode = 'song_level/'#(song_level/dynamic)\n",
    "full_data_path = dataset_path + annotations_path + rating_mode + time_continuity_mode\n",
    "## Here is the path to the audio recordings :\n",
    "audio_path = 'MEMD_audio/'\n",
    "full_audio_path = dataset_path + audio_path\n",
    "## Here is the path to where we write the csv\n",
    "csv_write_path = dataset_path + 'emotion_by_song/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['workerID', ' SongId', ' Valence', ' Arousal'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0           2\n",
       "1           2\n",
       "2           2\n",
       "3           2\n",
       "4           2\n",
       "5           2\n",
       "6           2\n",
       "7           2\n",
       "8           2\n",
       "9           2\n",
       "10          3\n",
       "11          3\n",
       "12          3\n",
       "13          3\n",
       "14          3\n",
       "15          3\n",
       "16          3\n",
       "17          3\n",
       "18          3\n",
       "19          3\n",
       "20          4\n",
       "21          4\n",
       "22          4\n",
       "23          4\n",
       "24          4\n",
       "25          4\n",
       "26          4\n",
       "27          4\n",
       "28          4\n",
       "29          4\n",
       "         ... \n",
       "17434    1998\n",
       "17435    1998\n",
       "17436    1998\n",
       "17437    1998\n",
       "17438    1998\n",
       "17439    1998\n",
       "17440    1998\n",
       "17441    1998\n",
       "17442    1998\n",
       "17443    1998\n",
       "17444    1999\n",
       "17445    1999\n",
       "17446    1999\n",
       "17447    1999\n",
       "17448    1999\n",
       "17449    1999\n",
       "17450    1999\n",
       "17451    1999\n",
       "17452    1999\n",
       "17453    1999\n",
       "17454    2000\n",
       "17455    2000\n",
       "17456    2000\n",
       "17457    2000\n",
       "17458    2000\n",
       "17459    2000\n",
       "17460    2000\n",
       "17461    2000\n",
       "17462    2000\n",
       "17463    2000\n",
       "Name:  SongId, Length: 17464, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data exploration for per song annotation\n",
    "s_annotation_path = full_data_path + 'static_annotations_songs_1_2000.csv'\n",
    "s_annotation = pd.read_csv(s_annotation_path)\n",
    "print(s_annotation.keys())\n",
    "s_annotation[' SongId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['song_id', 'sample_15000ms', 'sample_15500ms', 'sample_16000ms',\n",
       "       'sample_16500ms', 'sample_17000ms', 'sample_17500ms', 'sample_18000ms',\n",
       "       'sample_18500ms', 'sample_19000ms',\n",
       "       ...\n",
       "       'sample_621500ms', 'sample_622000ms', 'sample_622500ms',\n",
       "       'sample_623000ms', 'sample_623500ms', 'sample_624000ms',\n",
       "       'sample_624500ms', 'sample_625000ms', 'sample_625500ms',\n",
       "       'sample_626000ms'],\n",
       "      dtype='object', length=1224)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arousal data loading\n",
    "arousal_dataframe = pd.read_csv(full_data_path + 'arousal' + '.csv')\n",
    "arousal_dataframe.keys()\n",
    "arousal_dataframe = arousal_dataframe.drop(columns='sample_626500ms')\n",
    "arousal_dataframe.keys()\n",
    "#print(arousal_dataframe.shape)\n",
    "#print(arousal_dataframe['song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['song_id', 'sample_15000ms', 'sample_15500ms', 'sample_16000ms',\n",
       "       'sample_16500ms', 'sample_17000ms', 'sample_17500ms', 'sample_18000ms',\n",
       "       'sample_18500ms', 'sample_19000ms',\n",
       "       ...\n",
       "       'sample_621500ms', 'sample_622000ms', 'sample_622500ms',\n",
       "       'sample_623000ms', 'sample_623500ms', 'sample_624000ms',\n",
       "       'sample_624500ms', 'sample_625000ms', 'sample_625500ms',\n",
       "       'sample_626000ms'],\n",
       "      dtype='object', length=1224)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence_dataframe = pd.read_csv(full_data_path + 'valence' + '.csv')\n",
    "valence_dataframe.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotional_trajectory(song_id):\n",
    "    valence_id = (valence_dataframe.loc[valence_dataframe['song_id'] == song_id]\n",
    "                                   .drop(columns='song_id')\n",
    "                                   .values\n",
    "                 )\n",
    "    valence_id = np.ravel(valence_id)\n",
    "    \n",
    "    arousal_id = (arousal_dataframe.loc[arousal_dataframe['song_id'] == song_id]\n",
    "                                   .drop(columns='song_id')\n",
    "                                   .values\n",
    "                 )\n",
    "    arousal_id = np.ravel(arousal_id)\n",
    "    plt.figure()\n",
    "    plt.plot(valence_id, arousal_id)\n",
    "    plt.show()\n",
    "\n",
    "def compute_mfb_coef(sig, sample_rate):\n",
    "    return logfbank(signal = sig,\n",
    "                    samplerate=sample_rate,\n",
    "                    winlen=0.020,\n",
    "                    winstep=0.010,\n",
    "                    nfilt=40,\n",
    "                    nfft=512,\n",
    "                    lowfreq=0,\n",
    "                    highfreq=None,\n",
    "                    preemph=0.97)\n",
    "\n",
    "def slice_signal_by_time_steps(sig, sample_rate, time_step):\n",
    "    index_step = int(time_step*sample_rate)\n",
    "    n_time_steps = sig.shape[0]//index_step\n",
    "    return sig[:n_time_steps*index_step].reshape((n_time_steps, index_step))\n",
    "\n",
    "def audio_from_song_id(song_id):\n",
    "        file_path = full_audio_path + str(song_id) + '.mp3'\n",
    "        return librosa.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_id = 2\n",
    "valence_id = np.ravel(valence_dataframe.loc[valence_dataframe['song_id'] == song_id]\n",
    "                               .drop(columns='song_id')\n",
    "                               .values\n",
    "                     )\n",
    "\n",
    "def interpolate2d_emotion(valence_id, arousal_id, cur_step, target_step):\n",
    "    n = valence_id.shape[0]\n",
    "    x_cur = cur_step*np.arange(n)\n",
    "    #univariate spline interpolation of both lists\n",
    "    spl_valence = scipy.interpolate.UnivariateSpline(x_cur, valence_id)\n",
    "    spl_arousal = scipy.interpolate.UnivariateSpline(x_cur, arousal_id)\n",
    "\n",
    "    \n",
    "    x_target = target_step * np.arange(np.ceil(n*cur_step/target_step))\n",
    "    \n",
    "    \n",
    "    return spl_valence(x_target), spl_arousal(x_target)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84ea6ddbda34d37a37d45c6f29f0756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1802), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "error",
     "evalue": "failed in converting 2nd argument `y' of dfitpack.fpcurf0 to C/Fortran array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-cc382ea6c24c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                            \u001b[0marousal_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marousal_line_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                            \u001b[0mcur_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                                            target_step=100)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0marousal_interpolated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marousal_interpolated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-d50e6fabb6f3>\u001b[0m in \u001b[0;36minterpolate2d_emotion\u001b[0;34m(valence_id, arousal_id, cur_step, target_step)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#univariate spline interpolation of both lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mspl_valence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnivariateSpline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalence_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mspl_arousal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnivariateSpline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marousal_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/audio/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, w, bbox, k, s, ext, check_finite)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         data = dfitpack.fpcurf0(x, y, k, w=w, xb=bbox[0],\n\u001b[0;32m--> 186\u001b[0;31m                                 xe=bbox[1], s=s)\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;31m# nest too small, setting to maximum bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: failed in converting 2nd argument `y' of dfitpack.fpcurf0 to C/Fortran array"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for i in tqdm_notebook(range(arousal_dataframe.shape[0])):\n",
    "    arousal_line = arousal_dataframe.loc[i]\n",
    "    valence_line = valence_dataframe.loc[i]\n",
    "    song_id_ar = int(arousal_line['song_id'])\n",
    "    song_id_val = int(valence_line['song_id'])\n",
    "    if song_id_ar == song_id_val:\n",
    "        #Load the corresponding signal\n",
    "        y, sampling_rate = audio_from_song_id(song_id_ar)\n",
    "        #format it so that each row of the resulting matrix correspond to a 100 ms signal duration\n",
    "        formatted_signal = slice_signal_by_time_steps(y, sampling_rate, 100*1e-3)\n",
    "        #Remove the 30 first rows for which we don't have arousal/valence information\n",
    "        formatted_signal = formatted_signal[150:,:]\n",
    "        M = formatted_signal.shape[0]\n",
    "        n_mfb = 40\n",
    "        n_time_win = 9\n",
    "        mfb_coefficients = np.zeros((M, n_time_win, n_mfb))\n",
    "        for i in range(M):\n",
    "            mfb_coefficients[i,:,:] = compute_mfb_coef(formatted_signal[i], sampling_rate)\n",
    "        \n",
    "        mfb_coefficients -= np.mean(np.mean(mfb_coefficients, axis=0), axis=0)\n",
    "        \n",
    "        arousal_line_vector = np.ravel(arousal_line.drop('song_id').values)\n",
    "        arousal_line_vector = arousal_line_vector[~np.isnan(arousal_line_vector)]\n",
    "        valence_line_vector = np.ravel(valence_line.drop('song_id').values)\n",
    "        valence_line_vector = valence_line_vector[~np.isnan(valence_line_vector)]\n",
    "        arousal_interpolated, valence_interpolated = interpolate2d_emotion(valence_id=valence_line_vector,\n",
    "                                                                           arousal_id=arousal_line_vector,\n",
    "                                                                           cur_step=500,\n",
    "                                                                           target_step=100)\n",
    "\n",
    "        arousal_interpolated = arousal_interpolated[:M]\n",
    "        valence_interpolated = valence_interpolated[:M]\n",
    "        \n",
    "        mfb_coefficients = mfb_coefficients.reshape(M, n_mfb*n_time_win)\n",
    "        \n",
    "        emotional_vector = np.stack([arousal_interpolated, valence_interpolated]).T\n",
    "        \n",
    "        #save results\n",
    "        outputfile = feature_path + 'song_' + str(song_id_ar) + '.npz'\n",
    "        np.savez(outputfile, mfb = mfb_coefficients, emotion = emotional_vector)\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
